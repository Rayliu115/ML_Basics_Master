{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks with PyTorch\n",
    "\n",
    "\"Deep Learning\" is a general term that usually refers to the use of neural networks with multiple layers that synthesize the way the human brain learns and makes decisions. A convolutional neural network is a kind of neural network that extracts *features* from matrices of numeric values (often images) by convolving multiple filters over the matrix values to apply weights and identify patterns, such as edges, corners, and so on in an image. The numeric representations of these patterns are then passed to a fully-connected neural network layer to map the features to specific classes.\n",
    "\n",
    "There are several commonly used frameworks for creating CNNs. In this notebook, we'll build a simple example CNN using PyTorch.\n",
    "\n",
    "## Import libraries\n",
    "\n",
    "First, let's install and import the PyTorch libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported - ready to use PyTorch 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Other libraries we'll use\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data\n",
    "\n",
    "In this exercise, you'll train a CNN-based classification model that can classify images of geometric shapes. Let's take a look at the classes of shape the model needs to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 classes:\n",
      "['circle', 'square', 'triangle']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The images are in the data/shapes folder\n",
    "data_path = 'data/shapes/'\n",
    "\n",
    "# Get the class names\n",
    "#classes = os.listdir(data_path)\n",
    "classes =['circle', 'square', 'triangle'] \n",
    "classes.sort()\n",
    "print(len(classes), 'classes:')\n",
    "print(classes)\n",
    "\n",
    "# Show the first image in each folder\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "i = 0\n",
    "for sub_dir in classes:\n",
    "    i+=1\n",
    "    img_file = os.listdir(os.path.join(data_path,sub_dir))[0]\n",
    "    img_path = os.path.join(data_path, sub_dir, img_file)\n",
    "    img = mpimg.imread(img_path)\n",
    "    a=fig.add_subplot(1, len(classes),i)\n",
    "    a.axis('off')\n",
    "    imgplot = plt.imshow(img)\n",
    "    a.set_title(img_file)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "PyTorch includes functions for loading and transforming data. We'll use these to create an iterative loader for training data, and a second iterative loader for test data (which we'll use to validate the trained model). The loaders will transform the image data into *tensors*, which are the core data structure used in PyTorch, and normalize them so that the pixel values are in a scale with a mean of 0.5 and a standard deviation of 0.5.\n",
    "\n",
    "Run the following cell to define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready\n"
     ]
    }
   ],
   "source": [
    "# Function to ingest data using training and test loaders\n",
    "def load_dataset(data_path):\n",
    "    # Load all of the images\n",
    "    transformation = transforms.Compose([\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transformation\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# Get the iterative dataloaders for test and training data\n",
    "train_loader, test_loader = load_dataset(data_path)\n",
    "print('Data loaders ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CNN\n",
    "\n",
    "In PyTorch, you define a neural network model as a class that is derived from the **nn.Module** base class. Your class must define the layers in your network, and provide a **forward** method that is used to process data through the layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model class defined!\n"
     ]
    }
   ],
   "source": [
    "# Create a neural net class\n",
    "class Net(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Our images are RGB, so input channels = 3. We'll apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We'll apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 12 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A third convolutional layer takes 12 inputs and generates 24 outputs\n",
    "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # So our feature tensors are now 32 x 32, and we've generated 24 of them\n",
    "        # We need to flatten these and feed them to a fully-connected layer\n",
    "        # to map them to  the probability for each class\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use a relu activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "      \n",
    "        # Use a relu activation function after layer 2 (convolution 2 and pool)\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        \n",
    "        # Select some features to drop after the 3rd convolution to prevent overfitting\n",
    "        x = F.relu(self.drop(self.conv3(x)))\n",
    "        \n",
    "        # Only drop the features if this is a training pass\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return log_softmax tensor \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "print(\"CNN model class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now that we've defined a class for the network, we can train it using the image data.\n",
    "\n",
    "Training consists of an iterative series of forward passes in which the training data is processed in batches by the layers in the network, and the optimizer goes back and adjusts the weights. We'll also use a separate set of test images to test the model at the end of each iteration (or *epoch*) so we can track the performance improvement as the training process progresses.\n",
    "\n",
    "In the example below, we use 5 epochs to train the model using the batches of images loaded by the data loaders, holding back the data in the test data loader for validation. After each epoch, a loss function measures the error (*loss*) in the model and adjusts the weights (which were randomly generated for the first iteration) to try to improve accuracy. \n",
    "\n",
    "> **Note**: We're only using 5 epochs to minimize the training time for this simple example. A real-world CNN is usually trained over more epochs than this. CNN model training is processor-intensive, involving a lot of matrix and vector-based operations; so it's recommended to perform this on a system that can leverage GPUs, which are optimized for these kinds of calculation. This will take a while to complete on a CPU-based system - status will be displayed as the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (50x24576 and 6144x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m         test_loss \u001b[38;5;241m=\u001b[39m test(model, device, test_loader)\n\u001b[1;32m     98\u001b[0m         epoch_nums\u001b[38;5;241m.\u001b[39mappend(epoch)\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Push the data forward through the model layers\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Get the loss\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_criteria(output, target)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-tf210-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-tf210-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m24\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Feed to fully-connected layer to predict class\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Return log_softmax tensor \u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-tf210-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-tf210-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-tf210-torch/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (50x24576 and 6144x3)"
     ]
    }
   ],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "        \n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print metrics for every 10 batches so we see some progress\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Training set [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n",
    "                batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss\n",
    "            \n",
    "            \n",
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data) \n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss/batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss\n",
    "    \n",
    "    \n",
    "# Now use the train and test functions to train and test the model    \n",
    "\n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "print('Training on', device)\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(classes)).to(device)\n",
    "\n",
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "# (see https://pytorch.org/docs/stable/optim.html#algorithms for details of supported algorithms)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Train over 5 epochs (in a real scenario, you'd likely use many more)\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the loss history\n",
    "\n",
    "We tracked average training and validation loss for each epoch. We can plot these to verify that loss reduced as the model was trained, and to detect *over-fitting* (which is indicated by a continued drop in training loss after validation loss has levelled out or started to increase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(epoch_nums, training_loss)\n",
    "plt.plot(epoch_nums, validation_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance\n",
    "\n",
    "You can see the final accuracy based on the test data, but typically you'll want to explore performance metrics in a little more depth. Let's plot a confusion matrix to see how well the model is predicting each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from test set...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (50x24576 and 6144x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m target\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy():\n\u001b[1;32m     13\u001b[0m         truelabels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     15\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mappend(prediction) \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Plot the confusion matrix\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-tf210-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-tf210-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m24\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Feed to fully-connected layer to predict class\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Return log_softmax tensor \u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-tf210-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-tf210-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py-tf210-torch/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (50x24576 and 6144x3)"
     ]
    }
   ],
   "source": [
    "# Pytorch doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set the model to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "# Get predictions for the test data and convert to numpy arrays for use with SciKit-Learn\n",
    "print(\"Getting predictions from test set...\")\n",
    "truelabels = []\n",
    "predictions = []\n",
    "for data, target in test_loader:\n",
    "    for label in target.cpu().data.numpy():\n",
    "        truelabels.append(label)\n",
    "    for prediction in model.cpu()(data).data.numpy().argmax(1):\n",
    "        predictions.append(prediction) \n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(truelabels, predictions)\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel(\"Predicted Shape\")\n",
    "plt.ylabel(\"Actual Shape\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Trained model\n",
    "\n",
    "Now that you've trained a working model, you can save it (including the trained weights) for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "model_file = 'models/shape_classifier.pt'\n",
    "torch.save(model.state_dict(), model_file)\n",
    "del model\n",
    "print('model saved as', model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the trained model\n",
    "\n",
    "Now that we've trained and evaluated our model, we can use it to predict classes for new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from random import randint\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Function to predict the class of an image\n",
    "def predict_image(classifier, image):\n",
    "    import numpy\n",
    "    \n",
    "    # Set the classifer model to evaluation mode\n",
    "    classifier.eval()\n",
    "    \n",
    "    # Apply the same transformations as we did for the training images\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Preprocess the image\n",
    "    image_tensor = transformation(image).float()\n",
    "\n",
    "    # Add an extra batch dimension since pytorch treats all inputs as batches\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "\n",
    "    # Turn the input into a Variable\n",
    "    input_features = Variable(image_tensor)\n",
    "\n",
    "    # Predict the class of the image\n",
    "    output = classifier(input_features)\n",
    "    index = output.data.numpy().argmax()\n",
    "    return index\n",
    "\n",
    "\n",
    "# Function to create a random image (of a square, circle, or triangle)\n",
    "def create_image (size, shape):\n",
    "    from random import randint\n",
    "    import numpy as np\n",
    "    from PIL import Image, ImageDraw\n",
    "    \n",
    "    xy1 = randint(10,40)\n",
    "    xy2 = randint(60,100)\n",
    "    col = (randint(0,200), randint(0,200), randint(0,200))\n",
    "\n",
    "    img = Image.new(\"RGB\", size, (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    if shape == 'circle':\n",
    "        draw.ellipse([(xy1,xy1), (xy2,xy2)], fill=col)\n",
    "    elif shape == 'triangle':\n",
    "        draw.polygon([(xy1,xy1), (xy2,xy2), (xy2,xy1)], fill=col)\n",
    "    else: # square\n",
    "        draw.rectangle([(xy1,xy1), (xy2,xy2)], fill=col)\n",
    "    del draw\n",
    "    \n",
    "    return np.array(img)\n",
    "\n",
    "# Create a random test image\n",
    "classnames = os.listdir(os.path.join('data', 'shapes'))\n",
    "classnames.sort()\n",
    "shape = classnames[randint(0, len(classnames)-1)]\n",
    "img = create_image ((128,128), shape)\n",
    "\n",
    "# Display the image\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "\n",
    "# Create a new model class and load the saved weights\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "# Call the predction function\n",
    "index = predict_image(model, img)\n",
    "print(classes[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "To learn more about training convolutional neural networks with PyTorch, see the [PyTorch documentation](https://pytorch.org/).\n",
    "\n",
    "## Challenge: Safari Image Classification\n",
    "\n",
    "Hopefully this notebook has shown you the main steps in training and evaluating a CNN. Why not put what you've learned into practice with our Safari image classification challenge in the [/challenges/05 - Safari CNN Challenge.ipynb](./challenges/05%20-%20Safari%20CNN%20Challenge.ipynb) notebook?\n",
    "\n",
    "> **Note**: The time to complete this optional challenge is not included in the estimated time for this exercise - you can spend as little or as much time on it as you like!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-tf210-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
